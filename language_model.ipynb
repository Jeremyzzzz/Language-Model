{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model (Rule-based & Probability-based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 基于规则的语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "mission_grammar = '''\n",
    "mission = 人名 ， 方式 去 地点 结尾词\n",
    "人名 = Kitty | Jeremy | Ben | Alice\n",
    "方式 = 骑马 | 坐车 | 弹射 | 游泳\n",
    "地点 = 月球 | 火星 | 市中心 | 地心\n",
    "结尾词 = 吗？ | 吧！\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_grammar = '''\n",
    "sentence = NP VP\n",
    "NP = Det Adj* Noun\n",
    "Adj* = null | Adj Adj*\n",
    "VP = V NP | V NP PP\n",
    "PP = 在 NP Direction\n",
    "Det = 这个 | 那个 | 这些 | 那些 | 一个 \n",
    "Adj = 搞笑的 | 美丽的 | 胖胖的 | 湖南的 | 昏暗的\n",
    "Noun = 女孩 | 男孩 | 公司 | 长凳 | 湖面 | 健身房\n",
    "V = 看见 | 打 | 洗脸 | 起床 | 跑向\n",
    "P = 在\n",
    "Direction = 上 | 下 | 里 | 外\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {key: list}\n",
    "def create_grammar(grammar_str, split='=', line_split='\\n'):\n",
    "    grammar = {}\n",
    "    for line in grammar_str.split('\\n'):\n",
    "        if not line.strip(): continue\n",
    "        l = line.strip().split(split)\n",
    "        grammar[l[0].strip()] = [form.split() for form in l[1].split('|')]\n",
    "    return grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mission': [['人名', '，', '方式', '去', '地点', '结尾词']],\n",
       " '人名': [['Kitty'], ['Jeremy'], ['Ben'], ['Alice']],\n",
       " '方式': [['骑马'], ['坐车'], ['弹射'], ['游泳']],\n",
       " '地点': [['月球'], ['火星'], ['市中心'], ['地心']],\n",
       " '结尾词': [['吗？'], ['吧！']]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammar1 = create_grammar(mission_grammar)\n",
    "grammar1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': [['NP', 'VP']],\n",
       " 'NP': [['Det', 'Adj*', 'Noun']],\n",
       " 'Adj*': [['null'], ['Adj', 'Adj*']],\n",
       " 'VP': [['V', 'NP'], ['V', 'NP', 'PP']],\n",
       " 'PP': [['在', 'NP', 'Direction']],\n",
       " 'Det': [['这个'], ['那个'], ['这些'], ['那些'], ['一个']],\n",
       " 'Adj': [['搞笑的'], ['美丽的'], ['胖胖的'], ['湖南的'], ['昏暗的']],\n",
       " 'Noun': [['女孩'], ['男孩'], ['公司'], ['长凳'], ['湖面'], ['健身房']],\n",
       " 'V': [['看见'], ['打'], ['洗脸'], ['起床'], ['跑向']],\n",
       " 'P': [['在']],\n",
       " 'Direction': [['上'], ['下'], ['里'], ['外']]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammar2 = create_grammar(sentence_grammar)\n",
    "grammar2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(target, grammar):\n",
    "    # base case\n",
    "    if target not in grammar: return target\n",
    "    expanded = [generate(p, grammar) for p in random.choice(grammar[target])]\n",
    "    return ''.join([e for e in expanded if e != 'null'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alice，弹射去地心吧！'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate('mission', grammar1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alice，骑马去火星吧！'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate('mission', grammar1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'这些美丽的男孩打一个美丽的湖面'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate('sentence', grammar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'那些女孩打这个湖面在一个昏暗的搞笑的男孩上'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate('sentence', grammar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n(n, grammar):\n",
    "    sentences = []\n",
    "    if grammar == grammar1:\n",
    "        target = 'mission'\n",
    "    elif grammar == grammar2:\n",
    "        target = 'sentence'\n",
    "    for i in range(n):\n",
    "        sen = generate(target, grammar)\n",
    "        sentences.append(sen)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ben，坐车去市中心吧！',\n",
       " 'Jeremy，坐车去地心吧！',\n",
       " 'Alice，游泳去月球吗？',\n",
       " 'Jeremy，弹射去火星吧！',\n",
       " 'Alice，骑马去地心吗？',\n",
       " 'Alice，坐车去市中心吗？',\n",
       " 'Kitty，骑马去月球吧！',\n",
       " 'Kitty，游泳去地心吧！',\n",
       " 'Kitty，弹射去月球吧！',\n",
       " 'Jeremy，坐车去火星吧！']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_n(10, grammar1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['那个健身房打这个男孩在一个长凳上',\n",
       " '这些湖面起床这些湖南的女孩',\n",
       " '这个湖南的美丽的搞笑的昏暗的昏暗的美丽的公司跑向这个胖胖的搞笑的胖胖的男孩在那个湖南的男孩上',\n",
       " '这个湖面洗脸这些长凳在那个湖面外',\n",
       " '这个美丽的美丽的搞笑的健身房跑向那个长凳',\n",
       " '一个女孩洗脸这些女孩在那个公司里',\n",
       " '那个女孩打那些湖面在一个女孩上',\n",
       " '这个长凳看见一个湖面在这个昏暗的搞笑的长凳里',\n",
       " '那个女孩打一个搞笑的昏暗的搞笑的健身房',\n",
       " '那个湖南的昏暗的昏暗的健身房起床这个搞笑的胖胖的胖胖的昏暗的搞笑的美丽的男孩在那些长凳上',\n",
       " '那些胖胖的搞笑的湖面看见一个公司在那些湖南的男孩里',\n",
       " '那些胖胖的湖南的美丽的男孩看见一个湖面',\n",
       " '那个湖南的女孩打这些搞笑的湖南的女孩',\n",
       " '这些昏暗的美丽的长凳跑向那个健身房在这个胖胖的女孩里',\n",
       " '这个昏暗的搞笑的女孩打那个胖胖的湖面在那些搞笑的湖南的男孩外',\n",
       " '那些胖胖的健身房打一个昏暗的胖胖的男孩',\n",
       " '一个昏暗的昏暗的胖胖的男孩看见那个搞笑的长凳',\n",
       " '那个昏暗的女孩跑向一个公司在这个公司里',\n",
       " '那个男孩打一个公司在一个搞笑的搞笑的湖面下',\n",
       " '这个女孩洗脸一个女孩']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_n(20, grammar2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 基于概率的语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选用数据集：豆瓣评论数据集：https://github.com/Computing-Intelligence/datasource/raw/master/movie_comments.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './movie_comments.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = pd.read_csv(filename, dtype={\"id\":\"str\", \"link\":\"str\", \"name\":\"str\", \"comment\":\"str\", \"star\":\"str\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>中二得很</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                        link name  \\\n",
       "0  1  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "1  2  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "2  3  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "3  4  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "4  5  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "\n",
       "                                             comment star  \n",
       "0                                 吴京意淫到了脑残的地步，看了恶心想吐    1  \n",
       "1  首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...    2  \n",
       "2  吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...    2  \n",
       "3                      凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。    4  \n",
       "4                                               中二得很    1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'有一场坦克戏，简直令人浮想连连啊，吴京真的不是故意的嘛。又是压人压成肉泥的镜头，又是吴京站在坦克正前面，“稍有常识的人都会看出，如果敌方的铁骑继续前进”，然而吴京这个螳臂当车的歹徒真的阻挡住了。'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = content['comment'].tolist()\n",
    "comments[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261497"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(string):\n",
    "    return re.findall('\\w+', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/3k/8sn49nld4l1681x984tl8fx40000gn/T/jieba.cache\n",
      "Loading model cost 0.561 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "with_jieba_cut = Counter(jieba.cut(comments[99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('，', 6),\n",
       " ('的', 5),\n",
       " ('有', 2),\n",
       " ('坦克', 2),\n",
       " ('吴京', 2),\n",
       " ('真的', 2),\n",
       " ('。', 2),\n",
       " ('又', 2),\n",
       " ('是', 2),\n",
       " ('一场', 1)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_jieba_cut.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'有一场坦克戏简直令人浮想连连啊吴京真的不是故意的嘛又是压人压成肉泥的镜头又是吴京站在坦克正前面稍有常识的人都会看出如果敌方的铁骑继续前进然而吴京这个螳臂当车的歹徒真的阻挡住了'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(token(comments[99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_clean = [''.join(token(str(c)))for c in comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261497"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(s): return list(jieba.cut(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n"
     ]
    }
   ],
   "source": [
    "for i, line in enumerate(comments_clean):\n",
    "    if i % 10000 == 0: print(i)\n",
    "    if i > 1000000: break    \n",
    "    TOKEN += cut(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('的', 656524),\n",
       " ('了', 204840),\n",
       " ('是', 146212),\n",
       " ('我', 100676),\n",
       " ('都', 72510),\n",
       " ('很', 69424),\n",
       " ('看', 68044),\n",
       " ('电影', 67350),\n",
       " ('也', 64130),\n",
       " ('和', 62580)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count = Counter(TOKEN)\n",
    "words_count.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_1(word):\n",
    "    return words_count[word] / len(TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0074994772079362846"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_1('电影')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.191105163493057e-05"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_1('吴京')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0018005426347784664"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_1('好看')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = [str(t) for t in TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_2_GRAM = [''.join(TOKEN[i:i+2]) for i in range(len(TOKEN[:-2]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count_2 = Counter(TOKEN_2_GRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_2(word1, word2):\n",
    "    if word1 + word2 in words_count_2:\n",
    "        return words_count_2[word1+word2] / len(TOKEN_2_GRAM)\n",
    "    else:\n",
    "        return 1 / len(TOKEN_2_GRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00011691837894560556"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_2('这个','电影')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00021446171223736793"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_2('很','好看')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1135083709105291e-07"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_2('上升','很多')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-gram language model\n",
    "def get_probability(sentence):\n",
    "    words = cut(sentence)\n",
    "    sentence_pro = 1\n",
    "    for i, word in enumerate(words[:-1]):\n",
    "        next_ = words[i+1]\n",
    "        prob = prob_2(word, next_)\n",
    "        sentence_pro *= prob\n",
    "    return sentence_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0670683975356854e-25"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probability('这部电影的剧情非常有逻辑')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.157987711085333e-12"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probability('剧情非常狗血')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0063916949932695e-33"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probability('国产动漫的水平已经提升了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3014172348295621e-15"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probability('非常好看的一部电影')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 获得最优质的的语言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们能够生成随机的语言并且能判断之后，我们就可以生成更加合理的语言了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_best(grammar, language_model, n): # you code here\n",
    "    sentences = []\n",
    "    for sentence in generate_n(n, grammar):\n",
    "        sen_prob = language_model(sentence)\n",
    "        sentences.append((sentence, sen_prob))\n",
    "        print('sentence: {} with Prb: {}'.format(sentence, sen_prob))\n",
    "    best = sorted(sentences, key=lambda x: x[1], reverse=True)[0]\n",
    "    print('bset sentence: {} with Prb: {}'.format(best[0], best[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: Kitty，骑马去月球吧！ with Prb: 7.624667486243888e-42\n",
      "sentence: Ben，坐车去月球吧！ with Prb: 1.7118567954745383e-35\n",
      "sentence: Ben，坐车去市中心吧！ with Prb: 1.7118567954745383e-35\n",
      "sentence: Alice，骑马去市中心吗？ with Prb: 3.812333743121944e-42\n",
      "sentence: Ben，骑马去市中心吗？ with Prb: 3.812333743121944e-42\n",
      "sentence: Alice，弹射去火星吧！ with Prb: 2.2874002458731664e-41\n",
      "sentence: Ben，坐车去火星吗？ with Prb: 1.7118567954745383e-35\n",
      "sentence: Alice，游泳去火星吧！ with Prb: 2.2874002458731664e-41\n",
      "sentence: Ben，游泳去月球吗？ with Prb: 3.812333743121944e-42\n",
      "sentence: Ben，坐车去火星吧！ with Prb: 1.027114077284723e-34\n",
      "sentence: Alice，游泳去火星吧！ with Prb: 2.2874002458731664e-41\n",
      "sentence: Alice，骑马去火星吗？ with Prb: 7.624667486243888e-42\n",
      "sentence: Kitty，弹射去地心吗？ with Prb: 1.906166871560972e-42\n",
      "sentence: Jeremy，弹射去地心吗？ with Prb: 1.906166871560972e-42\n",
      "sentence: Ben，弹射去市中心吧！ with Prb: 1.906166871560972e-42\n",
      "sentence: Kitty，骑马去市中心吧！ with Prb: 3.812333743121944e-42\n",
      "sentence: Kitty，骑马去火星吧！ with Prb: 4.5748004917463327e-41\n",
      "sentence: Kitty，弹射去火星吗？ with Prb: 3.812333743121944e-42\n",
      "sentence: Alice，骑马去地心吗？ with Prb: 3.812333743121944e-42\n",
      "sentence: Kitty，游泳去月球吧！ with Prb: 3.812333743121944e-42\n",
      "bset sentence: Ben，坐车去火星吧！ with Prb: 1.027114077284723e-34\n"
     ]
    }
   ],
   "source": [
    "generate_best(grammar1, get_probability, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: 那些湖南的健身房跑向那个胖胖的胖胖的昏暗的湖面在这些湖面上 with Prb: 7.70537020414425e-100\n",
      "sentence: 一个湖面跑向这些健身房 with Prb: 3.4237135909490766e-34\n",
      "sentence: 这些湖面看见这些美丽的美丽的湖南的湖南的美丽的湖南的长凳在这个昏暗的公司上 with Prb: 1.5942864441492488e-132\n",
      "sentence: 这些搞笑的湖面洗脸一个美丽的搞笑的湖面 with Prb: 7.181839749952818e-57\n",
      "sentence: 这些公司洗脸这个男孩在这个公司上 with Prb: 2.5714423089141687e-51\n",
      "sentence: 一个健身房洗脸一个公司 with Prb: 9.22412533320104e-28\n",
      "sentence: 那些昏暗的公司跑向那个男孩在这些胖胖的胖胖的公司里 with Prb: 4.772739784724373e-82\n",
      "sentence: 这个湖面看见这个女孩在那些搞笑的美丽的湖南的湖南的胖胖的公司外 with Prb: 2.015823035800488e-104\n",
      "sentence: 那个女孩打这个男孩在一个男孩下 with Prb: 9.508946799902942e-45\n",
      "sentence: 那些美丽的女孩洗脸那些长凳 with Prb: 4.420019741775582e-37\n",
      "sentence: 一个公司洗脸那些搞笑的搞笑的女孩在一个公司上 with Prb: 3.1216140956088545e-67\n",
      "sentence: 这个长凳看见那些湖南的胖胖的公司在这个长凳上 with Prb: 1.420094466832168e-73\n",
      "sentence: 那些女孩打这些昏暗的胖胖的胖胖的长凳在那个搞笑的女孩上 with Prb: 2.677493948119926e-88\n",
      "sentence: 那些搞笑的健身房洗脸那些男孩 with Prb: 7.27393278187667e-39\n",
      "sentence: 一个男孩看见一个公司 with Prb: 1.1597376188489458e-19\n",
      "sentence: 那个健身房跑向这些湖面在那些长凳下 with Prb: 9.474228980369424e-61\n",
      "sentence: 那些美丽的男孩洗脸那个搞笑的湖面在一个搞笑的男孩下 with Prb: 3.254935881894161e-81\n",
      "sentence: 那个长凳看见一个搞笑的胖胖的美丽的昏暗的女孩 with Prb: 7.76120707123507e-65\n",
      "sentence: 这些昏暗的湖面打一个搞笑的女孩在那些女孩上 with Prb: 2.614437693435704e-72\n",
      "sentence: 那个健身房看见那个健身房在那个昏暗的健身房里 with Prb: 5.88903973183155e-66\n",
      "bset sentence: 一个男孩看见一个公司 with Prb: 1.1597376188489458e-19\n"
     ]
    }
   ],
   "source": [
    "generate_best(grammar2, get_probability, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 这个模型有什么问题？ 你准备如何提升？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: We need a large corpus to calculate probability and ususally the probability will depend on the source of the corpus. Also, long sentences will yeild to small probability because each probability is less than one and we only calculate 2-gram.\n",
    "We can use neural network to learn syntax and semantics to improve the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
